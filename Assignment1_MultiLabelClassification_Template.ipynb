{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Assignment1_MultiLabelClassification_Template.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganE510/Multi-label-Classsification/blob/patch1/Assignment1_MultiLabelClassification_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krCbrrtxPnKF",
        "colab_type": "text"
      },
      "source": [
        "# COMP47590: Advanced Machine Learning\n",
        "# Assignment 1: Multi-label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-dMOTRPnKJ",
        "colab_type": "text"
      },
      "source": [
        "Name(s): \n",
        "\n",
        "Student Number(s):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP3Wf9ecPnLh",
        "colab_type": "text"
      },
      "source": [
        "## Import Packages Etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQz3CfUPnLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn import svm\n",
        "from sklearn import ensemble\n",
        "from sklearn import linear_model\n",
        "from sklearn import neighbors\n",
        "from sklearn import neural_network\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# import other useful packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkdiI-8PnLq",
        "colab_type": "text"
      },
      "source": [
        "## Task 0: Load the Yeast Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVyilDAPnLr",
        "colab_type": "code",
        "outputId": "3fcf1e96-a7ff-411a-e4a1-e49eeb2d986e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "# Write your code here\n",
        "dataset = pd.read_csv('yeast.csv')\n",
        "display(dataset.head())"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Att1</th>\n",
              "      <th>Att2</th>\n",
              "      <th>Att3</th>\n",
              "      <th>Att4</th>\n",
              "      <th>Att5</th>\n",
              "      <th>Att6</th>\n",
              "      <th>Att7</th>\n",
              "      <th>Att8</th>\n",
              "      <th>Att9</th>\n",
              "      <th>Att10</th>\n",
              "      <th>Att11</th>\n",
              "      <th>Att12</th>\n",
              "      <th>Att13</th>\n",
              "      <th>Att14</th>\n",
              "      <th>Att15</th>\n",
              "      <th>Att16</th>\n",
              "      <th>Att17</th>\n",
              "      <th>Att18</th>\n",
              "      <th>Att19</th>\n",
              "      <th>Att20</th>\n",
              "      <th>Att21</th>\n",
              "      <th>Att22</th>\n",
              "      <th>Att23</th>\n",
              "      <th>Att24</th>\n",
              "      <th>Att25</th>\n",
              "      <th>Att26</th>\n",
              "      <th>Att27</th>\n",
              "      <th>Att28</th>\n",
              "      <th>Att29</th>\n",
              "      <th>Att30</th>\n",
              "      <th>Att31</th>\n",
              "      <th>Att32</th>\n",
              "      <th>Att33</th>\n",
              "      <th>Att34</th>\n",
              "      <th>Att35</th>\n",
              "      <th>Att36</th>\n",
              "      <th>Att37</th>\n",
              "      <th>Att38</th>\n",
              "      <th>Att39</th>\n",
              "      <th>Att40</th>\n",
              "      <th>...</th>\n",
              "      <th>Att78</th>\n",
              "      <th>Att79</th>\n",
              "      <th>Att80</th>\n",
              "      <th>Att81</th>\n",
              "      <th>Att82</th>\n",
              "      <th>Att83</th>\n",
              "      <th>Att84</th>\n",
              "      <th>Att85</th>\n",
              "      <th>Att86</th>\n",
              "      <th>Att87</th>\n",
              "      <th>Att88</th>\n",
              "      <th>Att89</th>\n",
              "      <th>Att90</th>\n",
              "      <th>Att91</th>\n",
              "      <th>Att92</th>\n",
              "      <th>Att93</th>\n",
              "      <th>Att94</th>\n",
              "      <th>Att95</th>\n",
              "      <th>Att96</th>\n",
              "      <th>Att97</th>\n",
              "      <th>Att98</th>\n",
              "      <th>Att99</th>\n",
              "      <th>Att100</th>\n",
              "      <th>Att101</th>\n",
              "      <th>Att102</th>\n",
              "      <th>Att103</th>\n",
              "      <th>Class1</th>\n",
              "      <th>Class2</th>\n",
              "      <th>Class3</th>\n",
              "      <th>Class4</th>\n",
              "      <th>Class5</th>\n",
              "      <th>Class6</th>\n",
              "      <th>Class7</th>\n",
              "      <th>Class8</th>\n",
              "      <th>Class9</th>\n",
              "      <th>Class10</th>\n",
              "      <th>Class11</th>\n",
              "      <th>Class12</th>\n",
              "      <th>Class13</th>\n",
              "      <th>Class14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004168</td>\n",
              "      <td>-0.170975</td>\n",
              "      <td>-0.156748</td>\n",
              "      <td>-0.142151</td>\n",
              "      <td>0.058781</td>\n",
              "      <td>0.026851</td>\n",
              "      <td>0.197719</td>\n",
              "      <td>0.041850</td>\n",
              "      <td>0.066938</td>\n",
              "      <td>-0.056617</td>\n",
              "      <td>-0.027230</td>\n",
              "      <td>-0.137411</td>\n",
              "      <td>0.067776</td>\n",
              "      <td>0.047175</td>\n",
              "      <td>0.155671</td>\n",
              "      <td>0.050766</td>\n",
              "      <td>0.102557</td>\n",
              "      <td>-0.020259</td>\n",
              "      <td>-0.200512</td>\n",
              "      <td>-0.095371</td>\n",
              "      <td>-0.081940</td>\n",
              "      <td>-0.103735</td>\n",
              "      <td>0.093299</td>\n",
              "      <td>0.105475</td>\n",
              "      <td>0.148560</td>\n",
              "      <td>0.085925</td>\n",
              "      <td>0.107879</td>\n",
              "      <td>0.108075</td>\n",
              "      <td>0.085388</td>\n",
              "      <td>0.124026</td>\n",
              "      <td>-0.003650</td>\n",
              "      <td>-0.127376</td>\n",
              "      <td>0.039394</td>\n",
              "      <td>-0.018364</td>\n",
              "      <td>0.050378</td>\n",
              "      <td>0.157190</td>\n",
              "      <td>0.203563</td>\n",
              "      <td>0.111552</td>\n",
              "      <td>0.017907</td>\n",
              "      <td>-0.001126</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.175325</td>\n",
              "      <td>-0.133636</td>\n",
              "      <td>0.005524</td>\n",
              "      <td>-0.014981</td>\n",
              "      <td>-0.031946</td>\n",
              "      <td>-0.015114</td>\n",
              "      <td>-0.047175</td>\n",
              "      <td>0.003829</td>\n",
              "      <td>0.010967</td>\n",
              "      <td>-0.006062</td>\n",
              "      <td>-0.027560</td>\n",
              "      <td>-0.019866</td>\n",
              "      <td>-0.024046</td>\n",
              "      <td>-0.025153</td>\n",
              "      <td>-0.009261</td>\n",
              "      <td>-0.025539</td>\n",
              "      <td>0.006166</td>\n",
              "      <td>-0.012976</td>\n",
              "      <td>-0.014259</td>\n",
              "      <td>-0.015024</td>\n",
              "      <td>-0.010747</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>-0.032056</td>\n",
              "      <td>-0.018312</td>\n",
              "      <td>0.030126</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.103956</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.098986</td>\n",
              "      <td>-0.054501</td>\n",
              "      <td>-0.007970</td>\n",
              "      <td>0.049113</td>\n",
              "      <td>-0.030580</td>\n",
              "      <td>-0.077933</td>\n",
              "      <td>-0.080529</td>\n",
              "      <td>-0.016267</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.009885</td>\n",
              "      <td>-0.155843</td>\n",
              "      <td>-0.059522</td>\n",
              "      <td>-0.098836</td>\n",
              "      <td>-0.071141</td>\n",
              "      <td>-0.023494</td>\n",
              "      <td>-0.071200</td>\n",
              "      <td>0.027767</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>-0.003761</td>\n",
              "      <td>0.074600</td>\n",
              "      <td>0.053080</td>\n",
              "      <td>-0.008138</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>-0.111704</td>\n",
              "      <td>-0.140291</td>\n",
              "      <td>-0.063347</td>\n",
              "      <td>0.066767</td>\n",
              "      <td>-0.167073</td>\n",
              "      <td>-0.095567</td>\n",
              "      <td>-0.047209</td>\n",
              "      <td>0.082206</td>\n",
              "      <td>0.144445</td>\n",
              "      <td>0.086581</td>\n",
              "      <td>-0.111850</td>\n",
              "      <td>-0.086560</td>\n",
              "      <td>0.024942</td>\n",
              "      <td>-0.131539</td>\n",
              "      <td>0.080062</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>-0.020209</td>\n",
              "      <td>-0.077359</td>\n",
              "      <td>-0.045139</td>\n",
              "      <td>-0.074738</td>\n",
              "      <td>0.051846</td>\n",
              "      <td>0.009323</td>\n",
              "      <td>0.184332</td>\n",
              "      <td>0.420424</td>\n",
              "      <td>-0.090224</td>\n",
              "      <td>-0.090718</td>\n",
              "      <td>-0.035266</td>\n",
              "      <td>-0.046729</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>-0.066023</td>\n",
              "      <td>-0.051916</td>\n",
              "      <td>0.007680</td>\n",
              "      <td>0.027719</td>\n",
              "      <td>-0.085811</td>\n",
              "      <td>0.111123</td>\n",
              "      <td>0.050541</td>\n",
              "      <td>0.027565</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>-0.041471</td>\n",
              "      <td>-0.079758</td>\n",
              "      <td>0.017161</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.509949</td>\n",
              "      <td>0.401709</td>\n",
              "      <td>0.293799</td>\n",
              "      <td>0.087714</td>\n",
              "      <td>0.011686</td>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.006255</td>\n",
              "      <td>0.013646</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.024447</td>\n",
              "      <td>-0.040576</td>\n",
              "      <td>0.014326</td>\n",
              "      <td>-0.074968</td>\n",
              "      <td>0.141365</td>\n",
              "      <td>-0.015182</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.006893</td>\n",
              "      <td>0.003736</td>\n",
              "      <td>-0.020726</td>\n",
              "      <td>-0.044104</td>\n",
              "      <td>-0.052959</td>\n",
              "      <td>-0.085572</td>\n",
              "      <td>-0.061547</td>\n",
              "      <td>-0.029578</td>\n",
              "      <td>0.027700</td>\n",
              "      <td>-0.094310</td>\n",
              "      <td>-0.047721</td>\n",
              "      <td>-0.081589</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>-0.106684</td>\n",
              "      <td>-0.068873</td>\n",
              "      <td>-0.105225</td>\n",
              "      <td>-0.065414</td>\n",
              "      <td>-0.047722</td>\n",
              "      <td>-0.070723</td>\n",
              "      <td>-0.057425</td>\n",
              "      <td>-0.042024</td>\n",
              "      <td>-0.034122</td>\n",
              "      <td>-0.049606</td>\n",
              "      <td>0.015137</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002432</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>-0.083572</td>\n",
              "      <td>-0.096943</td>\n",
              "      <td>0.148457</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>0.130691</td>\n",
              "      <td>-0.032325</td>\n",
              "      <td>0.028612</td>\n",
              "      <td>-0.023051</td>\n",
              "      <td>-0.092214</td>\n",
              "      <td>-0.103336</td>\n",
              "      <td>0.138232</td>\n",
              "      <td>-0.100351</td>\n",
              "      <td>0.140423</td>\n",
              "      <td>0.110074</td>\n",
              "      <td>0.096277</td>\n",
              "      <td>-0.044932</td>\n",
              "      <td>-0.089470</td>\n",
              "      <td>-0.009162</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>0.308378</td>\n",
              "      <td>-0.028053</td>\n",
              "      <td>0.026710</td>\n",
              "      <td>-0.066565</td>\n",
              "      <td>-0.122352</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.119092</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>-0.002262</td>\n",
              "      <td>0.072254</td>\n",
              "      <td>0.044512</td>\n",
              "      <td>-0.051467</td>\n",
              "      <td>0.074686</td>\n",
              "      <td>-0.007670</td>\n",
              "      <td>0.079438</td>\n",
              "      <td>0.062184</td>\n",
              "      <td>-0.013027</td>\n",
              "      <td>0.045538</td>\n",
              "      <td>0.080412</td>\n",
              "      <td>-0.010042</td>\n",
              "      <td>0.013029</td>\n",
              "      <td>-0.071975</td>\n",
              "      <td>0.089818</td>\n",
              "      <td>-0.016129</td>\n",
              "      <td>0.033105</td>\n",
              "      <td>0.024275</td>\n",
              "      <td>0.040428</td>\n",
              "      <td>0.064248</td>\n",
              "      <td>0.225613</td>\n",
              "      <td>0.176576</td>\n",
              "      <td>0.015501</td>\n",
              "      <td>0.009491</td>\n",
              "      <td>-0.013684</td>\n",
              "      <td>-0.017633</td>\n",
              "      <td>0.085007</td>\n",
              "      <td>-0.056274</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>-0.062951</td>\n",
              "      <td>0.227151</td>\n",
              "      <td>0.165897</td>\n",
              "      <td>0.150224</td>\n",
              "      <td>0.065105</td>\n",
              "      <td>0.110891</td>\n",
              "      <td>0.048451</td>\n",
              "      <td>0.114726</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.111806</td>\n",
              "      <td>-0.154732</td>\n",
              "      <td>0.302807</td>\n",
              "      <td>0.340027</td>\n",
              "      <td>-0.093332</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>-0.010558</td>\n",
              "      <td>-0.039194</td>\n",
              "      <td>-0.041628</td>\n",
              "      <td>-0.077455</td>\n",
              "      <td>-0.008553</td>\n",
              "      <td>-0.022404</td>\n",
              "      <td>-0.106131</td>\n",
              "      <td>-0.103067</td>\n",
              "      <td>-0.083059</td>\n",
              "      <td>-0.089064</td>\n",
              "      <td>-0.083809</td>\n",
              "      <td>0.200354</td>\n",
              "      <td>-0.075716</td>\n",
              "      <td>0.196605</td>\n",
              "      <td>0.152758</td>\n",
              "      <td>-0.028484</td>\n",
              "      <td>-0.074207</td>\n",
              "      <td>-0.089227</td>\n",
              "      <td>-0.049913</td>\n",
              "      <td>-0.043893</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.042037</td>\n",
              "      <td>0.007054</td>\n",
              "      <td>-0.069483</td>\n",
              "      <td>0.081015</td>\n",
              "      <td>-0.048207</td>\n",
              "      <td>0.089446</td>\n",
              "      <td>-0.004947</td>\n",
              "      <td>0.064456</td>\n",
              "      <td>-0.133387</td>\n",
              "      <td>0.068878</td>\n",
              "      <td>-0.139371</td>\n",
              "      <td>0.041487</td>\n",
              "      <td>-0.058531</td>\n",
              "      <td>0.021264</td>\n",
              "      <td>-0.101382</td>\n",
              "      <td>0.021015</td>\n",
              "      <td>0.096572</td>\n",
              "      <td>-0.005136</td>\n",
              "      <td>0.111104</td>\n",
              "      <td>-0.008323</td>\n",
              "      <td>0.020210</td>\n",
              "      <td>-0.003967</td>\n",
              "      <td>0.039762</td>\n",
              "      <td>0.006744</td>\n",
              "      <td>-0.041730</td>\n",
              "      <td>-0.174533</td>\n",
              "      <td>-0.101343</td>\n",
              "      <td>-0.115674</td>\n",
              "      <td>0.328511</td>\n",
              "      <td>-0.108945</td>\n",
              "      <td>-0.160748</td>\n",
              "      <td>-0.120290</td>\n",
              "      <td>-0.148308</td>\n",
              "      <td>-0.082882</td>\n",
              "      <td>-0.127218</td>\n",
              "      <td>-0.167186</td>\n",
              "      <td>-0.143210</td>\n",
              "      <td>-0.118028</td>\n",
              "      <td>-0.297516</td>\n",
              "      <td>-0.160082</td>\n",
              "      <td>...</td>\n",
              "      <td>0.108388</td>\n",
              "      <td>0.095516</td>\n",
              "      <td>0.015942</td>\n",
              "      <td>0.087354</td>\n",
              "      <td>0.176911</td>\n",
              "      <td>-0.062311</td>\n",
              "      <td>0.117205</td>\n",
              "      <td>-0.048277</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>0.014850</td>\n",
              "      <td>-0.066453</td>\n",
              "      <td>-0.067962</td>\n",
              "      <td>-0.083653</td>\n",
              "      <td>-0.081130</td>\n",
              "      <td>-0.061469</td>\n",
              "      <td>0.023662</td>\n",
              "      <td>-0.060467</td>\n",
              "      <td>0.044351</td>\n",
              "      <td>-0.057209</td>\n",
              "      <td>0.028047</td>\n",
              "      <td>0.029661</td>\n",
              "      <td>-0.050026</td>\n",
              "      <td>0.023248</td>\n",
              "      <td>-0.061539</td>\n",
              "      <td>-0.035160</td>\n",
              "      <td>0.067834</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Att1      Att2      Att3      Att4  ...  Class11  Class12  Class13  Class14\n",
              "0  0.004168 -0.170975 -0.156748 -0.142151  ...        0        1        1        0\n",
              "1 -0.103956  0.011879 -0.098986 -0.054501  ...        0        0        0        0\n",
              "2  0.509949  0.401709  0.293799  0.087714  ...        0        1        1        0\n",
              "3  0.119092  0.004412 -0.002262  0.072254  ...        0        0        0        0\n",
              "4  0.042037  0.007054 -0.069483  0.081015  ...        0        0        0        0\n",
              "\n",
              "[5 rows x 117 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OblAyyybuuNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "9f650da2-14c4-4c01-a5f6-6db233fb0f43"
      },
      "source": [
        "# visualize the labelcount\n",
        "tag_dict ={}\n",
        "for i in range(num_classes):\n",
        "  tag_dict['class'+ i.__str__()] = dataset.iloc[:,103+i].value_counts()[1]\n",
        "labelcount = pd.DataFrame(list(tag_dict.items()),columns=['label', 'count'])\n",
        "labelcount.plot(x='label', y='count', kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEiCAYAAAAVoQJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n38wsIy76ZmYZcdE3M8uIi76ZWUb+P9+IBV38O+w7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8YdrX3W3MCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array(dataset.iloc[:,:103])\n",
        "functions = np.array(dataset.iloc[:,103:])\n",
        "data_train, data_test, function_train, function_test = train_test_split(data, functions, random_state=0, train_size = 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-ouF5iPnLv",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Implement the Binary Relevance Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ne8r36uPnLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryRelevanceClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status = status.T\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data, status)\n",
        "        \n",
        "        # Return the classifier\n",
        "        return self\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X))\n",
        "            \n",
        "        return np.array(pos_functions).T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9t-99gDPnL1",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUWt73KgPnL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BRUnderSample(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, under_sampling='undersampling', algorithm_model = 'BaggingClassifier'):\n",
        "        self.under_sampling = under_sampling\n",
        "        self.algorithm_model = algorithm_model\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):\n",
        "         # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "        \n",
        "        # Add an option to under-sample\n",
        "        rus = RandomUnderSampler(random_state=0)\n",
        "        \n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status = status.T\n",
        "\n",
        "            # Under-sample data and status\n",
        "            if self.under_sampling == 'undersampling':\n",
        "                temp_data, status = rus.fit_sample(data,status)\n",
        "            elif self.under_sampling == 'naturesampling':\n",
        "                temp_data, status = data, status\n",
        "            # algorithm choose\n",
        "            if self.algorithm_model == 'BaggingClassifier':\n",
        "              self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(temp_data, status)\n",
        "            elif self.algorithm_model == 'LogisticRegression':\n",
        "              self.relevances_[i] = BaggingClassifier(base_estimator = linear_model.LogisticRegression(),n_estimators=10).fit(temp_data, status)\n",
        "            elif self.algorithm_model == 'DecisionTree':\n",
        "              self.relevances_[i] = BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 200),n_estimators=10).fit(temp_data, status)\n",
        "            elif self.algorithm_model == 'KNN':\n",
        "              self.relevances_[i] = BaggingClassifier(base_estimator = neighbors.KNeighborsClassifier(n_neighbors =3),max_samples=0.1,max_features=0.5).fit(temp_data,status)\n",
        "\n",
        "        # Return the classifier\n",
        "        return self\n",
        "\n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "        \n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "\n",
        "        # Iterate all functioins to predict\n",
        "        for  i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpqXLrZnPnL7",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQaTWIxPnL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "1e2a2821-dfb4-47d6-d62e-20d2f80457cf"
      },
      "source": [
        "br = BinaryRelevanceClassifier()\n",
        "br.fit(data_train, function_train)\n",
        "function_pred = br.predict(data_test)\n",
        "\n",
        "# The fraction of labels that are incorrectly predicted\n",
        "brLoss = hamming_loss(function_test, function_pred)\n",
        "# F1 score: weighted average of the precision and recall\n",
        "brfScore = f1_score(function_test, function_pred, average='macro')\n",
        "\n",
        "bru = BRUnderSample()\n",
        "bru.fit(data_train, function_train)\n",
        "function_pred = bru.predict(data_test)\n",
        "\n",
        "# The fraction of labels that are incorrectly predicted\n",
        "bruLoss = hamming_loss(function_test, function_pred)\n",
        "# F1 score: weighted average of the precision and recall\n",
        "brufScore = f1_score(function_test, function_pred, average='macro')\n",
        "\n",
        "name_list = ['Hamming Loss','F1-Score']\n",
        "num_list = [brLoss,brfScore]\n",
        "num_list1 = [bruLoss,brufScore]\n",
        "x = list(range(len(num_list)))\n",
        "total,n = 0.8, 2\n",
        "width = total / n\n",
        "\n",
        "plt.bar(x, num_list, width = width, label ='without sampling',fc = 'y')\n",
        "for i in range(len(x)):\n",
        "    x[i] = x[i] + width\n",
        "plt.bar(x, num_list1, width = width, label ='with sampling',tick_label = name_list,fc = 'r')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3sws4xz0ZmYZ9/8BgUtB6qFwgskAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWhp2sQ0NBUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9cdae6ac-65f0-47e8-b03c-72356dca0b96"
      },
      "source": [
        "param_grid = [\n",
        " {'under_sampling': ['undersampling','naturesampling'],\n",
        " 'algorithm_model': ['BaggingClassifier','LogisticRegression','DecisionTree','KNN']}\n",
        "]\n",
        "my_tuned_model = GridSearchCV(BRUnderSample(), param_grid, cv=cv_folds, verbose = 2)\n",
        "my_tuned_model.fit(data_train, function_train)\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print(my_tuned_model.best_params_)\n",
        "model_tuned_params_list[\"Tuned model\"] = my_tuned_model.best_params_\n",
        "print(my_tuned_model.best_score_)\n",
        "\n",
        "function_pred = my_tuned_model.predict(data_test)\n",
        "\n",
        "# The fraction of labels that are incorrectly predicted\n",
        "tunedLoss = hamming_loss(function_test, function_pred)\n",
        "print(tunedLoss)\n",
        "# F1 score: weighted average of the precision and recall\n",
        "tunedfScore = f1_score(function_test, function_pred, average='macro')\n",
        "print(tunedfScore)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20769382133018496\n",
            "0.338098381256791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osr3Vh5JPnMA",
        "colab_type": "text"
      },
      "source": [
        "## Task 4: Implement the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0YwaWVJPnMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class ClassChainsClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "        data = data.iloc[:,:103]\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data,status_squeeze)\n",
        "            data = np.concatenate((data,np.reshape(status_squeeze,(data.shape[0],1))),axis=1)\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        j = 0\n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            j = 103+i\n",
        "            # if len(X) > 0:\n",
        "            pos_functions.append(self.relevances_[i].predict(X[:,:j]))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4TBzeIxPnMF",
        "colab_type": "text"
      },
      "source": [
        "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5BfHfFqPnMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2OOZlaXPnMK",
        "colab_type": "text"
      },
      "source": [
        "## Task 6: Reflect on the Performance of the Different Models Evaluated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNXlCepP2uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFsbjnWPnML",
        "colab_type": "text"
      },
      "source": [
        "*Write your reflection here (max 300 words)*\n"
      ]
    }
  ]
}