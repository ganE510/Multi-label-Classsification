{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Assignment1_MultiLabelClassification_Template.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verarararaya/Multi-label-Classsification/blob/patch-1/Assignment1_MultiLabelClassification_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krCbrrtxPnKF",
        "colab_type": "text"
      },
      "source": [
        "# COMP47590: Advanced Machine Learning\n",
        "# Assignment 1: Multi-label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-dMOTRPnKJ",
        "colab_type": "text"
      },
      "source": [
        "Name(s): \n",
        "\n",
        "Student Number(s):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP3Wf9ecPnLh",
        "colab_type": "text"
      },
      "source": [
        "## Import Packages Etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQz3CfUPnLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "# import other useful packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkdiI-8PnLq",
        "colab_type": "text"
      },
      "source": [
        "## Task 0: Load the Yeast Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVyilDAPnLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "54fdaf82-a644-4955-cd71-8d81d6f745b2"
      },
      "source": [
        "# Write your code here\n",
        "data_sampling_rate = 0.1\n",
        "cv_folds = 10\n",
        "dataset = pd.read_csv('yeast.csv')\n",
        "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
        "num_classes = 14\n",
        "display(dataset.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Att1</th>\n",
              "      <th>Att2</th>\n",
              "      <th>Att3</th>\n",
              "      <th>Att4</th>\n",
              "      <th>Att5</th>\n",
              "      <th>Att6</th>\n",
              "      <th>Att7</th>\n",
              "      <th>Att8</th>\n",
              "      <th>Att9</th>\n",
              "      <th>Att10</th>\n",
              "      <th>Att11</th>\n",
              "      <th>Att12</th>\n",
              "      <th>Att13</th>\n",
              "      <th>Att14</th>\n",
              "      <th>Att15</th>\n",
              "      <th>Att16</th>\n",
              "      <th>Att17</th>\n",
              "      <th>Att18</th>\n",
              "      <th>Att19</th>\n",
              "      <th>Att20</th>\n",
              "      <th>Att21</th>\n",
              "      <th>Att22</th>\n",
              "      <th>Att23</th>\n",
              "      <th>Att24</th>\n",
              "      <th>Att25</th>\n",
              "      <th>Att26</th>\n",
              "      <th>Att27</th>\n",
              "      <th>Att28</th>\n",
              "      <th>Att29</th>\n",
              "      <th>Att30</th>\n",
              "      <th>Att31</th>\n",
              "      <th>Att32</th>\n",
              "      <th>Att33</th>\n",
              "      <th>Att34</th>\n",
              "      <th>Att35</th>\n",
              "      <th>Att36</th>\n",
              "      <th>Att37</th>\n",
              "      <th>Att38</th>\n",
              "      <th>Att39</th>\n",
              "      <th>Att40</th>\n",
              "      <th>...</th>\n",
              "      <th>Att78</th>\n",
              "      <th>Att79</th>\n",
              "      <th>Att80</th>\n",
              "      <th>Att81</th>\n",
              "      <th>Att82</th>\n",
              "      <th>Att83</th>\n",
              "      <th>Att84</th>\n",
              "      <th>Att85</th>\n",
              "      <th>Att86</th>\n",
              "      <th>Att87</th>\n",
              "      <th>Att88</th>\n",
              "      <th>Att89</th>\n",
              "      <th>Att90</th>\n",
              "      <th>Att91</th>\n",
              "      <th>Att92</th>\n",
              "      <th>Att93</th>\n",
              "      <th>Att94</th>\n",
              "      <th>Att95</th>\n",
              "      <th>Att96</th>\n",
              "      <th>Att97</th>\n",
              "      <th>Att98</th>\n",
              "      <th>Att99</th>\n",
              "      <th>Att100</th>\n",
              "      <th>Att101</th>\n",
              "      <th>Att102</th>\n",
              "      <th>Att103</th>\n",
              "      <th>Class1</th>\n",
              "      <th>Class2</th>\n",
              "      <th>Class3</th>\n",
              "      <th>Class4</th>\n",
              "      <th>Class5</th>\n",
              "      <th>Class6</th>\n",
              "      <th>Class7</th>\n",
              "      <th>Class8</th>\n",
              "      <th>Class9</th>\n",
              "      <th>Class10</th>\n",
              "      <th>Class11</th>\n",
              "      <th>Class12</th>\n",
              "      <th>Class13</th>\n",
              "      <th>Class14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>-0.080833</td>\n",
              "      <td>0.101113</td>\n",
              "      <td>0.137155</td>\n",
              "      <td>0.345117</td>\n",
              "      <td>0.127294</td>\n",
              "      <td>0.068368</td>\n",
              "      <td>-0.169925</td>\n",
              "      <td>-0.141878</td>\n",
              "      <td>-0.060876</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.055050</td>\n",
              "      <td>0.175624</td>\n",
              "      <td>0.093474</td>\n",
              "      <td>0.012321</td>\n",
              "      <td>-0.102264</td>\n",
              "      <td>-0.063405</td>\n",
              "      <td>-0.127554</td>\n",
              "      <td>-0.138436</td>\n",
              "      <td>-0.085390</td>\n",
              "      <td>-0.046946</td>\n",
              "      <td>0.048530</td>\n",
              "      <td>0.063162</td>\n",
              "      <td>0.093134</td>\n",
              "      <td>0.121917</td>\n",
              "      <td>0.053903</td>\n",
              "      <td>0.000944</td>\n",
              "      <td>-0.044352</td>\n",
              "      <td>0.002694</td>\n",
              "      <td>0.002628</td>\n",
              "      <td>-0.075961</td>\n",
              "      <td>-0.019926</td>\n",
              "      <td>0.015730</td>\n",
              "      <td>-0.013897</td>\n",
              "      <td>0.120527</td>\n",
              "      <td>0.125536</td>\n",
              "      <td>-0.169421</td>\n",
              "      <td>-0.302210</td>\n",
              "      <td>-0.319813</td>\n",
              "      <td>0.060472</td>\n",
              "      <td>0.077395</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009528</td>\n",
              "      <td>-0.047131</td>\n",
              "      <td>-0.041581</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>-0.014105</td>\n",
              "      <td>-0.012156</td>\n",
              "      <td>0.031484</td>\n",
              "      <td>-0.005590</td>\n",
              "      <td>-0.017066</td>\n",
              "      <td>-0.002890</td>\n",
              "      <td>-0.030247</td>\n",
              "      <td>-0.014444</td>\n",
              "      <td>0.017489</td>\n",
              "      <td>0.011680</td>\n",
              "      <td>-0.062062</td>\n",
              "      <td>-0.042489</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.024294</td>\n",
              "      <td>0.056802</td>\n",
              "      <td>-0.052268</td>\n",
              "      <td>-0.010334</td>\n",
              "      <td>-0.040510</td>\n",
              "      <td>-0.032323</td>\n",
              "      <td>-0.037396</td>\n",
              "      <td>0.015472</td>\n",
              "      <td>0.110733</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1973</th>\n",
              "      <td>-0.254618</td>\n",
              "      <td>-0.208370</td>\n",
              "      <td>-0.285014</td>\n",
              "      <td>-0.060617</td>\n",
              "      <td>-0.114393</td>\n",
              "      <td>0.027029</td>\n",
              "      <td>-0.007543</td>\n",
              "      <td>-0.006910</td>\n",
              "      <td>-0.122353</td>\n",
              "      <td>-0.175038</td>\n",
              "      <td>-0.120385</td>\n",
              "      <td>-0.133700</td>\n",
              "      <td>-0.121167</td>\n",
              "      <td>0.077591</td>\n",
              "      <td>0.021833</td>\n",
              "      <td>0.017612</td>\n",
              "      <td>0.061845</td>\n",
              "      <td>-0.023731</td>\n",
              "      <td>-0.007848</td>\n",
              "      <td>0.016371</td>\n",
              "      <td>-0.085663</td>\n",
              "      <td>-0.125675</td>\n",
              "      <td>-0.011988</td>\n",
              "      <td>0.012607</td>\n",
              "      <td>-0.016970</td>\n",
              "      <td>-0.012460</td>\n",
              "      <td>-0.083215</td>\n",
              "      <td>-0.008501</td>\n",
              "      <td>-0.078666</td>\n",
              "      <td>-0.062988</td>\n",
              "      <td>-0.072046</td>\n",
              "      <td>-0.199627</td>\n",
              "      <td>0.060201</td>\n",
              "      <td>0.028805</td>\n",
              "      <td>0.089866</td>\n",
              "      <td>0.050434</td>\n",
              "      <td>0.081960</td>\n",
              "      <td>-0.029870</td>\n",
              "      <td>-0.029154</td>\n",
              "      <td>-0.101834</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.093056</td>\n",
              "      <td>-0.104694</td>\n",
              "      <td>-0.085030</td>\n",
              "      <td>-0.095274</td>\n",
              "      <td>-0.054181</td>\n",
              "      <td>0.009495</td>\n",
              "      <td>0.103368</td>\n",
              "      <td>0.067079</td>\n",
              "      <td>0.151858</td>\n",
              "      <td>0.067519</td>\n",
              "      <td>0.027682</td>\n",
              "      <td>0.099722</td>\n",
              "      <td>0.140477</td>\n",
              "      <td>0.014675</td>\n",
              "      <td>-0.047473</td>\n",
              "      <td>-0.109510</td>\n",
              "      <td>0.067391</td>\n",
              "      <td>0.007604</td>\n",
              "      <td>-0.030253</td>\n",
              "      <td>-0.063537</td>\n",
              "      <td>-0.084206</td>\n",
              "      <td>-0.093209</td>\n",
              "      <td>-0.022572</td>\n",
              "      <td>-0.112000</td>\n",
              "      <td>-0.101954</td>\n",
              "      <td>0.073823</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>-0.005527</td>\n",
              "      <td>0.059287</td>\n",
              "      <td>-0.000521</td>\n",
              "      <td>0.063776</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.073924</td>\n",
              "      <td>-0.010954</td>\n",
              "      <td>0.123185</td>\n",
              "      <td>0.036790</td>\n",
              "      <td>0.040247</td>\n",
              "      <td>0.057382</td>\n",
              "      <td>0.024894</td>\n",
              "      <td>0.056926</td>\n",
              "      <td>0.133109</td>\n",
              "      <td>0.105194</td>\n",
              "      <td>0.082346</td>\n",
              "      <td>0.094686</td>\n",
              "      <td>0.005630</td>\n",
              "      <td>0.058440</td>\n",
              "      <td>0.055901</td>\n",
              "      <td>0.127118</td>\n",
              "      <td>0.148569</td>\n",
              "      <td>0.089273</td>\n",
              "      <td>0.097951</td>\n",
              "      <td>0.069097</td>\n",
              "      <td>-0.035831</td>\n",
              "      <td>-0.019492</td>\n",
              "      <td>-0.044755</td>\n",
              "      <td>0.111824</td>\n",
              "      <td>-0.093819</td>\n",
              "      <td>-0.054755</td>\n",
              "      <td>-0.097503</td>\n",
              "      <td>0.051455</td>\n",
              "      <td>0.064326</td>\n",
              "      <td>0.117468</td>\n",
              "      <td>0.103409</td>\n",
              "      <td>0.145173</td>\n",
              "      <td>0.137443</td>\n",
              "      <td>0.125924</td>\n",
              "      <td>0.056854</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.174094</td>\n",
              "      <td>-0.156307</td>\n",
              "      <td>-0.033599</td>\n",
              "      <td>0.242864</td>\n",
              "      <td>-0.074522</td>\n",
              "      <td>-0.057204</td>\n",
              "      <td>-0.089951</td>\n",
              "      <td>-0.041770</td>\n",
              "      <td>-0.040696</td>\n",
              "      <td>-0.047097</td>\n",
              "      <td>-0.062489</td>\n",
              "      <td>-0.060333</td>\n",
              "      <td>-0.077719</td>\n",
              "      <td>-0.076192</td>\n",
              "      <td>-0.042088</td>\n",
              "      <td>0.133508</td>\n",
              "      <td>-0.048117</td>\n",
              "      <td>0.275593</td>\n",
              "      <td>-0.050168</td>\n",
              "      <td>0.257710</td>\n",
              "      <td>-0.054529</td>\n",
              "      <td>-0.046094</td>\n",
              "      <td>-0.067116</td>\n",
              "      <td>-0.060102</td>\n",
              "      <td>-0.019308</td>\n",
              "      <td>0.035031</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2298</th>\n",
              "      <td>-0.000127</td>\n",
              "      <td>-0.039124</td>\n",
              "      <td>-0.072420</td>\n",
              "      <td>0.100816</td>\n",
              "      <td>-0.120224</td>\n",
              "      <td>-0.039158</td>\n",
              "      <td>0.003486</td>\n",
              "      <td>-0.046203</td>\n",
              "      <td>-0.031848</td>\n",
              "      <td>0.030684</td>\n",
              "      <td>-0.081018</td>\n",
              "      <td>-0.032001</td>\n",
              "      <td>-0.028948</td>\n",
              "      <td>0.037193</td>\n",
              "      <td>-0.060299</td>\n",
              "      <td>0.034796</td>\n",
              "      <td>-0.009672</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>-0.063832</td>\n",
              "      <td>-0.029816</td>\n",
              "      <td>-0.010184</td>\n",
              "      <td>0.063836</td>\n",
              "      <td>0.007564</td>\n",
              "      <td>0.093128</td>\n",
              "      <td>0.010670</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.070199</td>\n",
              "      <td>0.058187</td>\n",
              "      <td>-0.020974</td>\n",
              "      <td>0.063375</td>\n",
              "      <td>0.047442</td>\n",
              "      <td>0.041008</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>-0.048090</td>\n",
              "      <td>-0.066061</td>\n",
              "      <td>-0.094176</td>\n",
              "      <td>-0.118203</td>\n",
              "      <td>-0.126008</td>\n",
              "      <td>-0.175311</td>\n",
              "      <td>-0.177175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.166689</td>\n",
              "      <td>-0.120625</td>\n",
              "      <td>-0.071145</td>\n",
              "      <td>-0.078217</td>\n",
              "      <td>0.111015</td>\n",
              "      <td>-0.086214</td>\n",
              "      <td>0.114843</td>\n",
              "      <td>0.108820</td>\n",
              "      <td>0.182903</td>\n",
              "      <td>0.191980</td>\n",
              "      <td>-0.099046</td>\n",
              "      <td>-0.062340</td>\n",
              "      <td>-0.099456</td>\n",
              "      <td>-0.078554</td>\n",
              "      <td>0.025578</td>\n",
              "      <td>-0.094762</td>\n",
              "      <td>0.056024</td>\n",
              "      <td>-0.078279</td>\n",
              "      <td>0.149759</td>\n",
              "      <td>-0.076545</td>\n",
              "      <td>-0.080236</td>\n",
              "      <td>-0.084317</td>\n",
              "      <td>0.110877</td>\n",
              "      <td>-0.097467</td>\n",
              "      <td>-0.075484</td>\n",
              "      <td>0.061949</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>-0.098985</td>\n",
              "      <td>-0.103361</td>\n",
              "      <td>0.299805</td>\n",
              "      <td>0.182854</td>\n",
              "      <td>0.157079</td>\n",
              "      <td>-0.004096</td>\n",
              "      <td>-0.097990</td>\n",
              "      <td>-0.160499</td>\n",
              "      <td>-0.174065</td>\n",
              "      <td>-0.111566</td>\n",
              "      <td>0.040665</td>\n",
              "      <td>0.224778</td>\n",
              "      <td>0.143717</td>\n",
              "      <td>0.024614</td>\n",
              "      <td>-0.014898</td>\n",
              "      <td>-0.162385</td>\n",
              "      <td>-0.025081</td>\n",
              "      <td>-0.176000</td>\n",
              "      <td>-0.141192</td>\n",
              "      <td>-0.133762</td>\n",
              "      <td>-0.184043</td>\n",
              "      <td>-0.015613</td>\n",
              "      <td>0.126715</td>\n",
              "      <td>0.186561</td>\n",
              "      <td>0.096724</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>-0.072579</td>\n",
              "      <td>-0.015396</td>\n",
              "      <td>-0.026754</td>\n",
              "      <td>-0.164367</td>\n",
              "      <td>-0.184245</td>\n",
              "      <td>-0.136293</td>\n",
              "      <td>-0.023429</td>\n",
              "      <td>0.133065</td>\n",
              "      <td>0.142327</td>\n",
              "      <td>-0.099569</td>\n",
              "      <td>-0.169127</td>\n",
              "      <td>-0.188286</td>\n",
              "      <td>0.125565</td>\n",
              "      <td>0.072134</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054616</td>\n",
              "      <td>-0.074107</td>\n",
              "      <td>0.019868</td>\n",
              "      <td>-0.012254</td>\n",
              "      <td>-0.045515</td>\n",
              "      <td>-0.013949</td>\n",
              "      <td>-0.056094</td>\n",
              "      <td>0.015482</td>\n",
              "      <td>-0.009718</td>\n",
              "      <td>-0.018856</td>\n",
              "      <td>-0.038609</td>\n",
              "      <td>0.013846</td>\n",
              "      <td>-0.017879</td>\n",
              "      <td>-0.009962</td>\n",
              "      <td>-0.009524</td>\n",
              "      <td>-0.034815</td>\n",
              "      <td>0.018225</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.024911</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>-0.014923</td>\n",
              "      <td>-0.005365</td>\n",
              "      <td>-0.026002</td>\n",
              "      <td>0.014528</td>\n",
              "      <td>0.012318</td>\n",
              "      <td>0.117208</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Att1      Att2      Att3  ...  Class12  Class13  Class14\n",
              "267  -0.080833  0.101113  0.137155  ...        0        0        0\n",
              "1973 -0.254618 -0.208370 -0.285014  ...        1        1        0\n",
              "432  -0.005527  0.059287 -0.000521  ...        1        1        0\n",
              "2298 -0.000127 -0.039124 -0.072420  ...        0        0        0\n",
              "1236 -0.098985 -0.103361  0.299805  ...        1        1        0\n",
              "\n",
              "[5 rows x 117 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-ouF5iPnLv",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Implement the Binary Relevance Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ne8r36uPnLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class BinaryRelevanceClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data, status_squeeze)\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X))\n",
        "            \n",
        "        return np.array(pos_functions).T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzbJT9-uhHm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "3f356d5b-eae3-43ab-e58d-d27bbec46a1e"
      },
      "source": [
        "my_model = BinaryRelevanceClassifier()\n",
        "my_model.fit(np.array(dataset.iloc[:40,:103]), np.array(dataset.iloc[:40,103:]))\n",
        "my_model.predict(np.array(dataset.iloc[41:49,:103]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9t-99gDPnL1",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUWt73KgPnL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class BinaryRelevanceClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False,under_sampling='undersampling'):\n",
        "        self.add_noise = add_noise\n",
        "        self.under_sampling = under_sampling\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions): \n",
        "        # Create a new empty dictionary into which we will store relevance \n",
        "        self.relevances_ = dict()  \n",
        "        if self.under_sampling == 'undersampling':\n",
        "          for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            rus = RandomUnderSampler(random_state=0)\n",
        "            X_resampled, y_resampled = rus.fit_sample(data,status_squeeze)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(X_resampled, y_resampled)\n",
        "        elif self.under_sampling == 'noundersampling':\n",
        "          for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data, status_squeeze)\n",
        "        \n",
        "\n",
        "\n",
        "        # Iterate all functioins\n",
        "        \n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TN_AKvfkeoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "5f4779bc-2dfa-4744-a9ea-60a6aad2db17"
      },
      "source": [
        "my_model = BinaryRelevanceClassifier(under_sampling='undersampling')\n",
        "my_model.fit(np.array(dataset.iloc[:40,:103]), np.array(dataset.iloc[:40,103:]))\n",
        "my_model.predict(np.array(dataset.iloc[41:49,:103]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
              "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpqXLrZnPnL7",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQaTWIxPnL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osr3Vh5JPnMA",
        "colab_type": "text"
      },
      "source": [
        "## Task 4: Implement the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0YwaWVJPnMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class ClassChainsClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            # print(status_squeeze.shape)\n",
        "            # print(type(functions))\n",
        "            # print(type(data))\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data, status_squeeze)\n",
        "            data = np.concatenate((data,np.reshape(status_squeeze,(data.shape[0],1))),axis=1)\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            j = 103+i\n",
        "            pos_functions.append(self.relevances_[i].predict(X[:,:j]))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po6Q39_frk4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "571769fc-8199-4155-f4ca-09d2a3e2c0d3"
      },
      "source": [
        "my_model = ClassChainsClassifier()\n",
        "my_model.fit(np.array(dataset.iloc[:40,:103]), np.array(dataset.iloc[:40,103:]))\n",
        "my_model.predict(np.array(dataset.iloc[41:49,:]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4TBzeIxPnMF",
        "colab_type": "text"
      },
      "source": [
        "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5BfHfFqPnMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2OOZlaXPnMK",
        "colab_type": "text"
      },
      "source": [
        "## Task 6: Reflect on the Performance of the Different Models Evaluated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNXlCepP2uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFsbjnWPnML",
        "colab_type": "text"
      },
      "source": [
        "*Write your reflection here (max 300 words)*\n"
      ]
    }
  ]
}