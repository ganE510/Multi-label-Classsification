{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Assignment1_MultiLabelClassification_Template.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganE510/Multi-label-Classsification/blob/patch1/Assignment1_MultiLabelClassification_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krCbrrtxPnKF",
        "colab_type": "text"
      },
      "source": [
        "# COMP47590: Advanced Machine Learning\n",
        "# Assignment 1: Multi-label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-dMOTRPnKJ",
        "colab_type": "text"
      },
      "source": [
        "Name(s): \n",
        "\n",
        "Student Number(s):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP3Wf9ecPnLh",
        "colab_type": "text"
      },
      "source": [
        "## Import Packages Etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQz3CfUPnLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# import other useful packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkdiI-8PnLq",
        "colab_type": "text"
      },
      "source": [
        "## Task 0: Load the Yeast Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVyilDAPnLr",
        "colab_type": "code",
        "outputId": "029cd7b8-8acd-49b6-e2c0-78fedc03433a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "# Write your code here\n",
        "dataset = pd.read_csv('yeast.csv')\n",
        "num_classes = 14\n",
        "display(dataset.head())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Att1</th>\n",
              "      <th>Att2</th>\n",
              "      <th>Att3</th>\n",
              "      <th>Att4</th>\n",
              "      <th>Att5</th>\n",
              "      <th>Att6</th>\n",
              "      <th>Att7</th>\n",
              "      <th>Att8</th>\n",
              "      <th>Att9</th>\n",
              "      <th>Att10</th>\n",
              "      <th>Att11</th>\n",
              "      <th>Att12</th>\n",
              "      <th>Att13</th>\n",
              "      <th>Att14</th>\n",
              "      <th>Att15</th>\n",
              "      <th>Att16</th>\n",
              "      <th>Att17</th>\n",
              "      <th>Att18</th>\n",
              "      <th>Att19</th>\n",
              "      <th>Att20</th>\n",
              "      <th>Att21</th>\n",
              "      <th>Att22</th>\n",
              "      <th>Att23</th>\n",
              "      <th>Att24</th>\n",
              "      <th>Att25</th>\n",
              "      <th>Att26</th>\n",
              "      <th>Att27</th>\n",
              "      <th>Att28</th>\n",
              "      <th>Att29</th>\n",
              "      <th>Att30</th>\n",
              "      <th>Att31</th>\n",
              "      <th>Att32</th>\n",
              "      <th>Att33</th>\n",
              "      <th>Att34</th>\n",
              "      <th>Att35</th>\n",
              "      <th>Att36</th>\n",
              "      <th>Att37</th>\n",
              "      <th>Att38</th>\n",
              "      <th>Att39</th>\n",
              "      <th>Att40</th>\n",
              "      <th>...</th>\n",
              "      <th>Att78</th>\n",
              "      <th>Att79</th>\n",
              "      <th>Att80</th>\n",
              "      <th>Att81</th>\n",
              "      <th>Att82</th>\n",
              "      <th>Att83</th>\n",
              "      <th>Att84</th>\n",
              "      <th>Att85</th>\n",
              "      <th>Att86</th>\n",
              "      <th>Att87</th>\n",
              "      <th>Att88</th>\n",
              "      <th>Att89</th>\n",
              "      <th>Att90</th>\n",
              "      <th>Att91</th>\n",
              "      <th>Att92</th>\n",
              "      <th>Att93</th>\n",
              "      <th>Att94</th>\n",
              "      <th>Att95</th>\n",
              "      <th>Att96</th>\n",
              "      <th>Att97</th>\n",
              "      <th>Att98</th>\n",
              "      <th>Att99</th>\n",
              "      <th>Att100</th>\n",
              "      <th>Att101</th>\n",
              "      <th>Att102</th>\n",
              "      <th>Att103</th>\n",
              "      <th>Class1</th>\n",
              "      <th>Class2</th>\n",
              "      <th>Class3</th>\n",
              "      <th>Class4</th>\n",
              "      <th>Class5</th>\n",
              "      <th>Class6</th>\n",
              "      <th>Class7</th>\n",
              "      <th>Class8</th>\n",
              "      <th>Class9</th>\n",
              "      <th>Class10</th>\n",
              "      <th>Class11</th>\n",
              "      <th>Class12</th>\n",
              "      <th>Class13</th>\n",
              "      <th>Class14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004168</td>\n",
              "      <td>-0.170975</td>\n",
              "      <td>-0.156748</td>\n",
              "      <td>-0.142151</td>\n",
              "      <td>0.058781</td>\n",
              "      <td>0.026851</td>\n",
              "      <td>0.197719</td>\n",
              "      <td>0.041850</td>\n",
              "      <td>0.066938</td>\n",
              "      <td>-0.056617</td>\n",
              "      <td>-0.027230</td>\n",
              "      <td>-0.137411</td>\n",
              "      <td>0.067776</td>\n",
              "      <td>0.047175</td>\n",
              "      <td>0.155671</td>\n",
              "      <td>0.050766</td>\n",
              "      <td>0.102557</td>\n",
              "      <td>-0.020259</td>\n",
              "      <td>-0.200512</td>\n",
              "      <td>-0.095371</td>\n",
              "      <td>-0.081940</td>\n",
              "      <td>-0.103735</td>\n",
              "      <td>0.093299</td>\n",
              "      <td>0.105475</td>\n",
              "      <td>0.148560</td>\n",
              "      <td>0.085925</td>\n",
              "      <td>0.107879</td>\n",
              "      <td>0.108075</td>\n",
              "      <td>0.085388</td>\n",
              "      <td>0.124026</td>\n",
              "      <td>-0.003650</td>\n",
              "      <td>-0.127376</td>\n",
              "      <td>0.039394</td>\n",
              "      <td>-0.018364</td>\n",
              "      <td>0.050378</td>\n",
              "      <td>0.157190</td>\n",
              "      <td>0.203563</td>\n",
              "      <td>0.111552</td>\n",
              "      <td>0.017907</td>\n",
              "      <td>-0.001126</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.175325</td>\n",
              "      <td>-0.133636</td>\n",
              "      <td>0.005524</td>\n",
              "      <td>-0.014981</td>\n",
              "      <td>-0.031946</td>\n",
              "      <td>-0.015114</td>\n",
              "      <td>-0.047175</td>\n",
              "      <td>0.003829</td>\n",
              "      <td>0.010967</td>\n",
              "      <td>-0.006062</td>\n",
              "      <td>-0.027560</td>\n",
              "      <td>-0.019866</td>\n",
              "      <td>-0.024046</td>\n",
              "      <td>-0.025153</td>\n",
              "      <td>-0.009261</td>\n",
              "      <td>-0.025539</td>\n",
              "      <td>0.006166</td>\n",
              "      <td>-0.012976</td>\n",
              "      <td>-0.014259</td>\n",
              "      <td>-0.015024</td>\n",
              "      <td>-0.010747</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>-0.032056</td>\n",
              "      <td>-0.018312</td>\n",
              "      <td>0.030126</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.103956</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.098986</td>\n",
              "      <td>-0.054501</td>\n",
              "      <td>-0.007970</td>\n",
              "      <td>0.049113</td>\n",
              "      <td>-0.030580</td>\n",
              "      <td>-0.077933</td>\n",
              "      <td>-0.080529</td>\n",
              "      <td>-0.016267</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.009885</td>\n",
              "      <td>-0.155843</td>\n",
              "      <td>-0.059522</td>\n",
              "      <td>-0.098836</td>\n",
              "      <td>-0.071141</td>\n",
              "      <td>-0.023494</td>\n",
              "      <td>-0.071200</td>\n",
              "      <td>0.027767</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>-0.003761</td>\n",
              "      <td>0.074600</td>\n",
              "      <td>0.053080</td>\n",
              "      <td>-0.008138</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>-0.111704</td>\n",
              "      <td>-0.140291</td>\n",
              "      <td>-0.063347</td>\n",
              "      <td>0.066767</td>\n",
              "      <td>-0.167073</td>\n",
              "      <td>-0.095567</td>\n",
              "      <td>-0.047209</td>\n",
              "      <td>0.082206</td>\n",
              "      <td>0.144445</td>\n",
              "      <td>0.086581</td>\n",
              "      <td>-0.111850</td>\n",
              "      <td>-0.086560</td>\n",
              "      <td>0.024942</td>\n",
              "      <td>-0.131539</td>\n",
              "      <td>0.080062</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>-0.020209</td>\n",
              "      <td>-0.077359</td>\n",
              "      <td>-0.045139</td>\n",
              "      <td>-0.074738</td>\n",
              "      <td>0.051846</td>\n",
              "      <td>0.009323</td>\n",
              "      <td>0.184332</td>\n",
              "      <td>0.420424</td>\n",
              "      <td>-0.090224</td>\n",
              "      <td>-0.090718</td>\n",
              "      <td>-0.035266</td>\n",
              "      <td>-0.046729</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>-0.066023</td>\n",
              "      <td>-0.051916</td>\n",
              "      <td>0.007680</td>\n",
              "      <td>0.027719</td>\n",
              "      <td>-0.085811</td>\n",
              "      <td>0.111123</td>\n",
              "      <td>0.050541</td>\n",
              "      <td>0.027565</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>-0.041471</td>\n",
              "      <td>-0.079758</td>\n",
              "      <td>0.017161</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.509949</td>\n",
              "      <td>0.401709</td>\n",
              "      <td>0.293799</td>\n",
              "      <td>0.087714</td>\n",
              "      <td>0.011686</td>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.006255</td>\n",
              "      <td>0.013646</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.024447</td>\n",
              "      <td>-0.040576</td>\n",
              "      <td>0.014326</td>\n",
              "      <td>-0.074968</td>\n",
              "      <td>0.141365</td>\n",
              "      <td>-0.015182</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.006893</td>\n",
              "      <td>0.003736</td>\n",
              "      <td>-0.020726</td>\n",
              "      <td>-0.044104</td>\n",
              "      <td>-0.052959</td>\n",
              "      <td>-0.085572</td>\n",
              "      <td>-0.061547</td>\n",
              "      <td>-0.029578</td>\n",
              "      <td>0.027700</td>\n",
              "      <td>-0.094310</td>\n",
              "      <td>-0.047721</td>\n",
              "      <td>-0.081589</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>-0.106684</td>\n",
              "      <td>-0.068873</td>\n",
              "      <td>-0.105225</td>\n",
              "      <td>-0.065414</td>\n",
              "      <td>-0.047722</td>\n",
              "      <td>-0.070723</td>\n",
              "      <td>-0.057425</td>\n",
              "      <td>-0.042024</td>\n",
              "      <td>-0.034122</td>\n",
              "      <td>-0.049606</td>\n",
              "      <td>0.015137</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002432</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>-0.083572</td>\n",
              "      <td>-0.096943</td>\n",
              "      <td>0.148457</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>0.130691</td>\n",
              "      <td>-0.032325</td>\n",
              "      <td>0.028612</td>\n",
              "      <td>-0.023051</td>\n",
              "      <td>-0.092214</td>\n",
              "      <td>-0.103336</td>\n",
              "      <td>0.138232</td>\n",
              "      <td>-0.100351</td>\n",
              "      <td>0.140423</td>\n",
              "      <td>0.110074</td>\n",
              "      <td>0.096277</td>\n",
              "      <td>-0.044932</td>\n",
              "      <td>-0.089470</td>\n",
              "      <td>-0.009162</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>0.308378</td>\n",
              "      <td>-0.028053</td>\n",
              "      <td>0.026710</td>\n",
              "      <td>-0.066565</td>\n",
              "      <td>-0.122352</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.119092</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>-0.002262</td>\n",
              "      <td>0.072254</td>\n",
              "      <td>0.044512</td>\n",
              "      <td>-0.051467</td>\n",
              "      <td>0.074686</td>\n",
              "      <td>-0.007670</td>\n",
              "      <td>0.079438</td>\n",
              "      <td>0.062184</td>\n",
              "      <td>-0.013027</td>\n",
              "      <td>0.045538</td>\n",
              "      <td>0.080412</td>\n",
              "      <td>-0.010042</td>\n",
              "      <td>0.013029</td>\n",
              "      <td>-0.071975</td>\n",
              "      <td>0.089818</td>\n",
              "      <td>-0.016129</td>\n",
              "      <td>0.033105</td>\n",
              "      <td>0.024275</td>\n",
              "      <td>0.040428</td>\n",
              "      <td>0.064248</td>\n",
              "      <td>0.225613</td>\n",
              "      <td>0.176576</td>\n",
              "      <td>0.015501</td>\n",
              "      <td>0.009491</td>\n",
              "      <td>-0.013684</td>\n",
              "      <td>-0.017633</td>\n",
              "      <td>0.085007</td>\n",
              "      <td>-0.056274</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>-0.062951</td>\n",
              "      <td>0.227151</td>\n",
              "      <td>0.165897</td>\n",
              "      <td>0.150224</td>\n",
              "      <td>0.065105</td>\n",
              "      <td>0.110891</td>\n",
              "      <td>0.048451</td>\n",
              "      <td>0.114726</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.111806</td>\n",
              "      <td>-0.154732</td>\n",
              "      <td>0.302807</td>\n",
              "      <td>0.340027</td>\n",
              "      <td>-0.093332</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>-0.010558</td>\n",
              "      <td>-0.039194</td>\n",
              "      <td>-0.041628</td>\n",
              "      <td>-0.077455</td>\n",
              "      <td>-0.008553</td>\n",
              "      <td>-0.022404</td>\n",
              "      <td>-0.106131</td>\n",
              "      <td>-0.103067</td>\n",
              "      <td>-0.083059</td>\n",
              "      <td>-0.089064</td>\n",
              "      <td>-0.083809</td>\n",
              "      <td>0.200354</td>\n",
              "      <td>-0.075716</td>\n",
              "      <td>0.196605</td>\n",
              "      <td>0.152758</td>\n",
              "      <td>-0.028484</td>\n",
              "      <td>-0.074207</td>\n",
              "      <td>-0.089227</td>\n",
              "      <td>-0.049913</td>\n",
              "      <td>-0.043893</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.042037</td>\n",
              "      <td>0.007054</td>\n",
              "      <td>-0.069483</td>\n",
              "      <td>0.081015</td>\n",
              "      <td>-0.048207</td>\n",
              "      <td>0.089446</td>\n",
              "      <td>-0.004947</td>\n",
              "      <td>0.064456</td>\n",
              "      <td>-0.133387</td>\n",
              "      <td>0.068878</td>\n",
              "      <td>-0.139371</td>\n",
              "      <td>0.041487</td>\n",
              "      <td>-0.058531</td>\n",
              "      <td>0.021264</td>\n",
              "      <td>-0.101382</td>\n",
              "      <td>0.021015</td>\n",
              "      <td>0.096572</td>\n",
              "      <td>-0.005136</td>\n",
              "      <td>0.111104</td>\n",
              "      <td>-0.008323</td>\n",
              "      <td>0.020210</td>\n",
              "      <td>-0.003967</td>\n",
              "      <td>0.039762</td>\n",
              "      <td>0.006744</td>\n",
              "      <td>-0.041730</td>\n",
              "      <td>-0.174533</td>\n",
              "      <td>-0.101343</td>\n",
              "      <td>-0.115674</td>\n",
              "      <td>0.328511</td>\n",
              "      <td>-0.108945</td>\n",
              "      <td>-0.160748</td>\n",
              "      <td>-0.120290</td>\n",
              "      <td>-0.148308</td>\n",
              "      <td>-0.082882</td>\n",
              "      <td>-0.127218</td>\n",
              "      <td>-0.167186</td>\n",
              "      <td>-0.143210</td>\n",
              "      <td>-0.118028</td>\n",
              "      <td>-0.297516</td>\n",
              "      <td>-0.160082</td>\n",
              "      <td>...</td>\n",
              "      <td>0.108388</td>\n",
              "      <td>0.095516</td>\n",
              "      <td>0.015942</td>\n",
              "      <td>0.087354</td>\n",
              "      <td>0.176911</td>\n",
              "      <td>-0.062311</td>\n",
              "      <td>0.117205</td>\n",
              "      <td>-0.048277</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>0.014850</td>\n",
              "      <td>-0.066453</td>\n",
              "      <td>-0.067962</td>\n",
              "      <td>-0.083653</td>\n",
              "      <td>-0.081130</td>\n",
              "      <td>-0.061469</td>\n",
              "      <td>0.023662</td>\n",
              "      <td>-0.060467</td>\n",
              "      <td>0.044351</td>\n",
              "      <td>-0.057209</td>\n",
              "      <td>0.028047</td>\n",
              "      <td>0.029661</td>\n",
              "      <td>-0.050026</td>\n",
              "      <td>0.023248</td>\n",
              "      <td>-0.061539</td>\n",
              "      <td>-0.035160</td>\n",
              "      <td>0.067834</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Att1      Att2      Att3      Att4  ...  Class11  Class12  Class13  Class14\n",
              "0  0.004168 -0.170975 -0.156748 -0.142151  ...        0        1        1        0\n",
              "1 -0.103956  0.011879 -0.098986 -0.054501  ...        0        0        0        0\n",
              "2  0.509949  0.401709  0.293799  0.087714  ...        0        1        1        0\n",
              "3  0.119092  0.004412 -0.002262  0.072254  ...        0        0        0        0\n",
              "4  0.042037  0.007054 -0.069483  0.081015  ...        0        0        0        0\n",
              "\n",
              "[5 rows x 117 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OblAyyybuuNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "0581482d-4f64-4363-cb49-ba3929744b0d"
      },
      "source": [
        "# visualize the labelcount\n",
        "tag_dict ={}\n",
        "for i in range(num_classes):\n",
        "  tag_dict['class'+ i.__str__()] = dataset.iloc[:,103+i].value_counts()[1]\n",
        "labelcount = pd.DataFrame(list(tag_dict.items()),columns=['label', 'count'])\n",
        "labelcount.plot(x='label', y='count', kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEiCAYAAAAVoQJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n38wsIy76ZmYZcdE3M8uIi76ZWUb+P9+IBV38O+w7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8YdrX3W3MCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dataset\n",
        "Y = np.array(dataset.iloc[:,103:])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, train_size = 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-ouF5iPnLv",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Implement the Binary Relevance Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ne8r36uPnLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class BinaryRelevanceClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data.iloc[:,:103], status_squeeze)\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X.iloc[:,:103]))\n",
        "            \n",
        "        return np.array(pos_functions).T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzbJT9-uhHm-",
        "colab_type": "code",
        "outputId": "75069f89-e49d-4473-ff2f-8e35747a87ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "my_model = BinaryRelevanceClassifier()\n",
        "my_model.fit(X_train, y_train)\n",
        "my_model.predict(X_test)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 1, 1, 0],\n",
              "       [0, 0, 1, ..., 1, 1, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9t-99gDPnL1",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUWt73KgPnL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class UnderBinaryRelevanceClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions): \n",
        "        # Create a new empty dictionary into which we will store relevance \n",
        "        self.relevances_ = dict()  \n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            rus = RandomUnderSampler(random_state=0)\n",
        "            X_resampled, y_resampled = rus.fit_sample(data.iloc[:,:103], status)\n",
        "            # print(X_resampled.shape)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(X_resampled, y_resampled)\n",
        "\n",
        "        # Iterate all functioins\n",
        "        \n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        \n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            pos_functions.append(self.relevances_[i].predict(X.iloc[:,:103]))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TN_AKvfkeoR",
        "colab_type": "code",
        "outputId": "203a117d-3850-421b-9cbe-7d3e61279d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "my_model = UnderBinaryRelevanceClassifier()\n",
        "my_model.fit(X_train, y_train)\n",
        "my_model.predict(X_test)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 1, 1, 1],\n",
              "       [0, 0, 1, ..., 0, 1, 1],\n",
              "       [1, 0, 0, ..., 1, 1, 1],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       [1, 0, 1, ..., 0, 0, 1],\n",
              "       [0, 0, 1, ..., 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpqXLrZnPnL7",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQaTWIxPnL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osr3Vh5JPnMA",
        "colab_type": "text"
      },
      "source": [
        "## Task 4: Implement the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0YwaWVJPnMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "class ClassChainsClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # Constructor for the classifier object\n",
        "    def __init__(self, add_noise = False):\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "    # The fit function to train a classifier\n",
        "    def fit(self, data, functions):    \n",
        "        # Create a new empty dictionary into which we will store relevance\n",
        "        self.relevances_ = dict()\n",
        "        data = data.iloc[:,:103]\n",
        "        # Iterate all functioins\n",
        "        for i in range(14):\n",
        "            status = functions[:,i]\n",
        "            status_squeeze = np.squeeze(status)\n",
        "            self.relevances_[i] = BaggingClassifier(n_estimators=10, random_state=0).fit(data,status_squeeze)\n",
        "            data = np.concatenate((data,np.reshape(status_squeeze,(data.shape[0],1))),axis=1)\n",
        "            \n",
        "    # The predict function to make a set of predictions for a set of query instances\n",
        "    def predict(self, X):\n",
        "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
        "        check_is_fitted(self, ['relevances_'])\n",
        "\n",
        "        # Initialise an empty list to store the predictions made\n",
        "        pos_functions = list()\n",
        "        j = 0\n",
        "        # Iterate all functioins to predict\n",
        "        for i in range(14):\n",
        "            j = 103+i\n",
        "            # if len(X) > 0:\n",
        "            pos_functions.append(self.relevances_[i].predict(X[:,:j]))\n",
        "            \n",
        "        return np.array(pos_functions).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "44020eca-339f-4342-a41f-8fde5815266f",
        "id": "OtAyFWYHHFPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "my_model = ClassChainsClassifier()\n",
        "print(X_train)\n",
        "my_model.fit(X_train, y_train)\n",
        "my_model.predict(X_test)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Att1      Att2      Att3  ...  Class12  Class13  Class14\n",
            "2067  0.110645  0.075770 -0.074272  ...        0        0        0\n",
            "936   0.067438 -0.086640  0.016308  ...        1        1        0\n",
            "376   0.106957 -0.042980  0.145034  ...        0        0        0\n",
            "2353 -0.054869 -0.014227  0.008956  ...        1        1        0\n",
            "1432  0.053625  0.029986 -0.034318  ...        1        1        0\n",
            "...        ...       ...       ...  ...      ...      ...      ...\n",
            "1033 -0.030665 -0.093305  0.018542  ...        1        1        0\n",
            "1731  0.116562 -0.016590  0.063544  ...        0        0        0\n",
            "763   0.051961 -0.044374  0.105554  ...        1        1        0\n",
            "835  -0.066107 -0.186666 -0.149487  ...        1        1        0\n",
            "1653  0.047349 -0.020428  0.149070  ...        0        0        0\n",
            "\n",
            "[1691 rows x 117 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-b5a78671e544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-c14c7c87b408>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m103\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# if len(X) > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpos_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 )\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), slice(None, 103, None))' is an invalid key"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4TBzeIxPnMF",
        "colab_type": "text"
      },
      "source": [
        "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5BfHfFqPnMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2OOZlaXPnMK",
        "colab_type": "text"
      },
      "source": [
        "## Task 6: Reflect on the Performance of the Different Models Evaluated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNXlCepP2uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFsbjnWPnML",
        "colab_type": "text"
      },
      "source": [
        "*Write your reflection here (max 300 words)*\n"
      ]
    }
  ]
}